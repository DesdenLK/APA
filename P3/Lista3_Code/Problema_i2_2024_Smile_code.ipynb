{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lista 3 - Problema 2 individual - Smile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Smile(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = torch.Tensor(data).float()\n",
    "        self.labels = torch.Tensor(labels).long()\n",
    "        self.n_classes = len(np.unique(labels))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.labels[index]\n",
    "\n",
    "        return x, F.one_hot(y, self.n_classes).float()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "# smile_train = torch.utils.data.DataLoader(Smile(X_train, y_train), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion para entrenar el modelo\n",
    "def train_loop(model, train, val, optimizer, patience=5, epochs=100):\n",
    "    \"\"\"_Bucle de entrenamiento_\n",
    "\n",
    "    Args:\n",
    "        model: red a entrenal\n",
    "        train: datos de entrenamiento\n",
    "        val: datos de validacion\n",
    "        optimizer: optimizador de pytorch, por ejemplo torch.optim.Adam\n",
    "        patience: numero de epochs sin mejora en el conjunto de validacion\n",
    "        epochs: numero de epochs\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    def epoch_loss(dataset):\n",
    "        data_loss = 0.0\n",
    "        for i, (data, labels) in enumerate(dataset):\n",
    "            inputs = data.to('cuda')\n",
    "            y = labels.to('cuda')\n",
    "            outputs = model(inputs)\n",
    "            loss = F.cross_entropy(outputs, y, reduction=\"mean\")\n",
    "            data_loss += loss.item()  \n",
    "        return data_loss / i  \n",
    "    \n",
    "    def early_stopping(val_loss, patience=5):\n",
    "        if len(val_loss) > patience:\n",
    "            if val_loss[-1] > val_loss[-(patience+1)]:\n",
    "                return True\n",
    "    \n",
    "    hist_loss = {'train': [], 'val': []}\n",
    "    pbar = tqdm(range(epochs))\n",
    "    for epoch in pbar:  # bucle para todos los epochs\n",
    "        for i, (data, labels) in enumerate(train):\n",
    "            # obtenemos los datos y los subimos a la GPU\n",
    "            inputs = data.to('cuda')\n",
    "            y = labels.to('cuda')\n",
    "\n",
    "            # Reiniciamos los gradientes\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Aplicamos los datos al modelo\n",
    "            outputs = model(inputs)\n",
    "            # Calculamos la perdida\n",
    "            loss = F.cross_entropy(outputs, y, reduction=\"mean\")\n",
    "\n",
    "            # Hacemos el paso hacia atras\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Calculamos la perdida en el conjunto de entrenamiento y validacion\n",
    "        with torch.no_grad():\n",
    "            hist_loss['train'].append(epoch_loss(train))\n",
    "            hist_loss['val'].append(epoch_loss(val))\n",
    "\n",
    "        # Mostramos la perdida en el conjunto de entrenamiento y validacion\n",
    "        pbar.set_postfix({'train': hist_loss['train'][-1], 'val': hist_loss['val'][-1]})\n",
    "\n",
    "        # Si la perdida en el conjunto de validacion no disminuye, paramos el entrenamiento\n",
    "        if early_stopping(hist_loss['val'], patience):\n",
    "            break\n",
    "            \n",
    "    return hist_loss \n",
    "\n",
    "# Para el optimizador podemos usar Adam, le pasaremos el siguiente objeto\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "# donde model es el modelo que queremos entrenar\n",
    "# y lr es la tasa de aprendizaje, 1e-4 es un valor comun, pero podeis probar otros valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clase para definir la arquitectura de la red convolucional\n",
    "class convolutional(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes=2,\n",
    "        input_size=32,\n",
    "        input_channels=3,\n",
    "        kernel_size=3,\n",
    "        kernels=[16, 32],\n",
    "        pooling=nn.MaxPool2d,\n",
    "        batch_norm=False,\n",
    "        dropout=0.0,\n",
    "    ):\n",
    "        super(convolutional, self).__init__()\n",
    "        nkernels = [input_channels] + kernels\n",
    "        padding = (kernel_size-1) // 2\n",
    "        self.convo = []\n",
    "        for k in range(1, len(nkernels)):\n",
    "            self.convo.append(\n",
    "                nn.Conv2d(\n",
    "                    nkernels[k - 1],\n",
    "                    nkernels[k],\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=1,\n",
    "                    padding=padding,\n",
    "                )\n",
    "            )\n",
    "            self.convo.append(nn.ReLU())\n",
    "            self.convo.append(pooling(kernel_size=2, stride=2))\n",
    "            if batch_norm:\n",
    "                self.convo.append(nn.BatchNorm2d(nkernels[k]))\n",
    "            if dropout > 0:\n",
    "                self.convo.append(nn.Dropout(dropout))\n",
    "        self.convo = nn.Sequential(*self.convo)\n",
    "        out_size = input_size // (2** len(kernels))\n",
    "        self.fc = nn.Linear(out_size * out_size * nkernels[-1], num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.convo(x)\n",
    "        return self.fc(out.view(out.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test):\n",
    "    \"\"\"_Funcion para obtener las predicciones de un modelo en un conjunto de test_\n",
    "\n",
    "    Poner el modelo en modo evaluacion antes de llamar a esta funcion\n",
    "    \n",
    "    Args:\n",
    "        model: _modelo entrenado_\n",
    "        test: _conjunto de test_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _etiquetas predichas, etiquetas reales_\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    true = []\n",
    "    for i, (data, labels) in enumerate(test):\n",
    "        inputs = data.to('cuda')\n",
    "        outputs = model(inputs)\n",
    "        preds.append(outputs.detach().cpu().numpy())\n",
    "        true.append(labels.detach().cpu().numpy())\n",
    "    return np.argmax(np.concatenate(preds), axis=1), np.argmax(np.concatenate(true), axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
