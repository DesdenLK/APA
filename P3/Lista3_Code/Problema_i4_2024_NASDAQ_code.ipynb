{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NASDAQ(Dataset):\n",
    "    def __init__(self, data, pred):\n",
    "        self.data = torch.Tensor(data).float()\n",
    "        self.pred = torch.Tensor(pred).float().unsqueeze(1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.pred[index]\n",
    "\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "#  BCN_train = torch.utils.data.DataLoader(BCNAir(X_train, y_train), batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion para entrenar el modelo\n",
    "def train_loop(model, train, val, optimizer, patience=5, epochs=100):\n",
    "    \"\"\"_Bucle de entrenamiento_\n",
    "\n",
    "    Args:\n",
    "        model: red a entrenal\n",
    "        optimizer: optimizador de pytorch, por ejemplo torch.optim.Adam\n",
    "        train: datos de entrenamiento\n",
    "        val: datos de validacion\n",
    "        epochs: numero de epochs\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    def epoch_loss(dataset):\n",
    "        data_loss = 0.0\n",
    "        for i, (data, labels) in enumerate(dataset):\n",
    "            inputs = data.to('cuda')\n",
    "            y = labels.to('cuda')\n",
    "            outputs = model(inputs)\n",
    "            loss = F.mse_loss(outputs, y, reduction=\"mean\")\n",
    "            data_loss += loss.item()  \n",
    "        return data_loss / i  \n",
    "    \n",
    "    def early_stopping(val_loss, patience=5):\n",
    "        if len(val_loss) > patience:\n",
    "            if val_loss[-1] > val_loss[-(patience+1)]:\n",
    "                return True\n",
    "    \n",
    "    hist_loss = {'train': [], 'val': []}\n",
    "    pbar = tqdm(range(epochs))\n",
    "    for epoch in pbar:  # bucle para todos los epochs\n",
    "        for i, (data, labels) in enumerate(train):\n",
    "            # obtenemos los datos y los subimos a la GPU\n",
    "            inputs = data.to('cuda')\n",
    "            y = labels.to('cuda')\n",
    "\n",
    "            # Reiniciamos los gradientes\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Aplicamos los datos al modelo\n",
    "            outputs = model(inputs)\n",
    "            # Calculamos la perdida\n",
    "            loss = F.mse_loss(outputs, y, reduction=\"mean\")\n",
    "\n",
    "            # Hacemos el paso hacia atras\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Calculamos la perdida en el conjunto de entrenamiento y validacion\n",
    "        with torch.no_grad():\n",
    "            hist_loss['train'].append(epoch_loss(train))\n",
    "            hist_loss['val'].append(epoch_loss(val))\n",
    "\n",
    "        # Mostramos la perdida en el conjunto de entrenamiento y validacion\n",
    "        pbar.set_postfix({'train': hist_loss['train'][-1], 'val': hist_loss['val'][-1]})\n",
    "\n",
    "        # Si la perdida en el conjunto de validacion no disminuye, paramos el entrenamiento\n",
    "        if early_stopping(hist_loss['val'], patience):\n",
    "            break\n",
    "            \n",
    "    return hist_loss \n",
    "\n",
    "# Para el optimizador podemos usar Adam, le pasaremos el siguiente objeto\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "# donde model es el modelo que queremos entrenar\n",
    "# y lr es la tasa de aprendizaje, 1e-4 es un valor comun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clase para definir la arquitectura del MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size,  hidden_layers_sizes, activation=nn.ReLU):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = []\n",
    "        self.fc1 = nn.Linear(input_size, hidden_layers_sizes[0])\n",
    "        self.layers.append(self.fc1)\n",
    "        self.layers.append(activation())\n",
    "        for i in range(1, len(hidden_layers_sizes)):\n",
    "            self.layers.append(nn.Linear(hidden_layers_sizes[i-1], hidden_layers_sizes[i]))\n",
    "            self.layers.append(activation())\n",
    "        self.layers = nn.Sequential(*self.layers)       \n",
    "        self.output = nn.Linear(hidden_layers_sizes[-1], 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.output(self.layers(x.view(x.size(0), -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test):\n",
    "    \"\"\"_Funcion para obtener las predicciones de un modelo en un conjunto de test_\n",
    "\n",
    "    Poner el modelo en modo evaluacion antes de llamar a esta funcion\n",
    "    \n",
    "    Args:\n",
    "        model: _modelo entrenado_\n",
    "        test: _conjunto de test_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _valores predichos, valores reales_\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    true = []\n",
    "    for i, (data, val) in enumerate(test):\n",
    "        inputs = data.to('cuda')\n",
    "        outputs = model(inputs)\n",
    "        preds.append(outputs.detach().cpu().numpy())\n",
    "        true.append(val.detach().cpu().numpy())\n",
    "    return np.concatenate(preds), np.concatenate(true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
